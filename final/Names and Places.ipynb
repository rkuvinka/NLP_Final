{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "title = []\n",
    "date = []\n",
    "report = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    if filename != '.DS_Store':\n",
    "        soup = BeautifulSoup(open(path + filename,encoding = \"ISO-8859-1\"),\"lxml\")\n",
    "        # get header (title + date)\n",
    "        letters = soup.find_all(class_=\"header\")[0].string\n",
    "        letters_split = letters.split(' ')\n",
    "        date_ = letters_split[-1]\n",
    "        title_ = ' '.join([i for i in letters_split[:-1] if i != ''])\n",
    "        # get report\n",
    "        # note: I did not remove '\\n' in case we will use it in the future (e.g. paragraphs).\n",
    "        tables = soup.find_all(\"table\", {\"cellpadding\": \"10\"})\n",
    "        text = tables[1].findAll('td')[0].text\n",
    "        text = text.replace('\\xa0', '')\n",
    "        title.append(title_)\n",
    "        date.append(date_)\n",
    "        report.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'title': title, 'date':date, 'report':report})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-10-16</td>\n",
       "      <td>\\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nWarm temperatur...</td>\n",
       "      <td>Junctions, Prospects, Blacktail Prospects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-11-16</td>\n",
       "      <td>\\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nBig change in t...</td>\n",
       "      <td>Prospect Peak, ? Wolves in Hayden, grizzlies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-12-16</td>\n",
       "      <td>\\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nClear sunny ski...</td>\n",
       "      <td>Canyons, Prospect, Junctions, Wapiti, ?wolves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-13-16</td>\n",
       "      <td>\\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nWell, you spend...</td>\n",
       "      <td>Prospect 996M Group, Junctions, Prospect Black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-14-16</td>\n",
       "      <td>\\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nWell, it truly ...</td>\n",
       "      <td>Lamar Canyon, grizzly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                                             report  \\\n",
       "0  10-10-16  \\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nWarm temperatur...   \n",
       "1  10-11-16  \\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nBig change in t...   \n",
       "2  10-12-16  \\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nClear sunny ski...   \n",
       "3  10-13-16  \\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nWell, you spend...   \n",
       "4  10-14-16  \\n\\n\\t\\t\\n\\t\\tHi Everyone ~\\n\\nWell, it truly ...   \n",
       "\n",
       "                                               title  \n",
       "0          Junctions, Prospects, Blacktail Prospects  \n",
       "1       Prospect Peak, ? Wolves in Hayden, grizzlies  \n",
       "2      Canyons, Prospect, Junctions, Wapiti, ?wolves  \n",
       "3  Prospect 996M Group, Junctions, Prospect Black...  \n",
       "4                              Lamar Canyon, grizzly  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RK/anaconda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "#The one that DOES NOT work\n",
    "\n",
    "import nltk.data, nltk.tag\n",
    "\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "patterns = [\n",
    "    \n",
    "#     (r'.*ing$','VBG'),\n",
    "#     (r'.*ed$','VBD'),\n",
    "#     (r'.*es$','VBZ'),\n",
    "#     (r'.*ould$','MD'),\n",
    "#     (r'.*\\'s$','NN$'),\n",
    "#     (r'.*s$','NNS'),\n",
    "#     (r'.*','NN')\n",
    "    \n",
    "    \n",
    "    \n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "         (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "         (r'.*able$', 'JJ'),                # adjectives\n",
    "         (r'.*ness$', 'NN'),                # nouns formed from adjectives  \n",
    "         (r'.*ly$', 'RB'),                  # adverbs\n",
    "         (r'.*s$', 'NNS'),                  # plural nouns  \n",
    "         (r'.*ing$', 'VBG'),                # gerunds   \n",
    "         (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "         (r'.*', 'NN')                      # nouns (default)\n",
    "        \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Geyser Basin': 'PLC', '996M': 'WLF', 'Prospect': 'WLFP', 'bear': 'BEAR', 'Hitching Post': 'PLC', 'Lamar': 'PLC', 'pup': 'WLF', 'Junction Buttes': 'WLFP', '925M': 'WLF', 'Mary Mountain Trail': 'PLC', 'Dead Puppy Hill': 'PLC', 'magpies': 'BIRD', 'Wapiti': 'ELK', 'Nine Mile': 'PLC', 'Divide Ridge': 'PLC', '949M': 'WLF', 'Cow Elk': 'Elk', '966M': 'WLF', '926F': 'WLF', 'ravens': 'BIRD', 'Buffalo Ford': 'PLC', 'coyotes': 'COY', '821F': 'WLF', 'Cache Valley': 'PLC', 'Canyons': 'WLFP', '763M': 'WLF', 'Mollies Pack': 'WLFP', '964M': 'WLF', 'grizzly': 'BEAR', 'Skyline': 'PLC', '969F': 'WLF', 'Antelope': 'PLC', 'Prospects': 'WLFP', 'Cub Creek': 'PLC', 'Coyote': 'PLC', 'Hubbard Hill': 'PLC', 'Junctions': 'WLFP', 'Hayden Valley': 'PLC', 'Round Prairie': 'PLC', 'Fountain Flats': 'PLC', 'South Entrance': 'PLC', 'Specimen Ridge': 'PLC', 'Long Pullout': 'PLC', \"Fishermen's\": 'PLC', 'Junction Lake': 'PLC', 'Cascade Meadow': 'PLC', 'Jasper Bench': 'PLC', 'moose': 'MOOSE', 'LittleT': 'WLF', '712M': 'WLF', 'Ranger Hill': 'PLC', 'Alum Creek': 'PLC', 'Boulder': 'PLC', 'Slough': 'PLC', 'pups': 'WLF', 'Elk Creek': 'PLC', 'Cascade Lake': 'PLC', 'Crystal Creek': 'PLC', '907F': 'WLF', \"Bob's Knob\": 'PLC', 'Hellroaring': 'PLC', 'bull': 'MOOSE', 'Mollies': 'WLFP', 'eagles': 'BIRD', 'Name': 'Type', 'Specimen': 'PLC', 'Lamar Canyon West': 'PLC', 'Blacktail': 'PLC', 'Footbridge': 'PLC', 'Peregrines': 'PLC', 'yearling': 'WLF', 'Little America': 'PLC', 'Prospect 996M Group': 'WLFP', 'Picnic': 'PLC', \"Fishermen's Pullout\": 'PLC', 'Black Bar': 'WLF', 'Confluence': 'PLC'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "mydict = {}\n",
    "reader = csv.reader(open(\"POS_tagging.csv\", \"r\"))\n",
    "for rows in reader:\n",
    "    k = rows[0]\n",
    "    v = rows[1]\n",
    "    mydict[k] = v\n",
    "print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(brown.words())\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words())\n",
    "most_freq_words = fd.most_common(10000)\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\n",
    "#default_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "#default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))          \n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph #1\n",
      "paragraph #2\n",
      "paragraph #3\n",
      "paragraph #4\n",
      "Nothing in Lamar this morning with only very faint signals on 926F and 949M.\n",
      "Things in report: {('949M', 'WLF'), ('926F', 'WLF')}\n",
      "Places in report: {'Lamar'} \n",
      "\n",
      "paragraph #5\n",
      "Junctions ~ The story was different over at Slough where there was a great chance that we would find the Junction Buttes. We spread out from Lamar Canyon West to Crystal Creek where Pauline and I heard a distant low howl. We went to Long Pullout and Rick found the Junctions at skyline south on Specimen just over the sheep cliff. They were just all bedded there happy snappy.\n",
      "Things in report: {('Junctions', 'WLFP')}\n",
      "Places in report: {'Lamar', 'Specimen', 'Slough'} \n",
      "\n",
      "paragraph #6\n",
      "Mid-morning the Junctions all got up and headed east on Specimen. Calvin went to Slough to look but none of us were able to find them again. \n",
      "Things in report: {('Junctions', 'WLFP')}\n",
      "Places in report: {'Specimen', 'Slough'} \n",
      "\n",
      "paragraph #7\n",
      "Prospect ~While standing there watching the Junctions, we could hear howling to the west on both sides of the road. It was not long before the Junctions were howling looking to the west. The Prospect wolves were on a carcass that is in the dried up pond west of Boulder in Little America. The carcass itself was out of view but we could see coyotes, eagles, ravens, and magpies. By the time we got there, the wolves had moved off of the carcass and had moved east to the north of Boulder turnout. We had alpha female 821F with her signature scar under her right eye, gray alpha male 966M who has lost his collar, 996M, the grayish black male, and 964M. Those five were howling to 2 gray pups and a black pup that we saw to the south. I am sure that they would have liked to cross but we just had too many people, so 821F crossed back over to join her pups.\n",
      "Things in report: {('pup', 'WLF'), ('996M', 'WLF'), ('964M', 'WLF'), ('Junctions', 'WLFP'), ('Prospect', 'WLFP'), ('966M', 'WLF'), ('pups', 'WLF'), ('821F', 'WLF')}\n",
      "Places in report: {'Boulder'} \n",
      "\n",
      "paragraph #8\n",
      "Later in the early afternoon, 996M and two others were seen at the carcass once more but , again, not for long before they were off to the west again. Dusty saw a gray in the Buffalo Ford and the others disappeared maybe into the Peregrines. It was 4:00, and we decided to hang up the spurs and head for the barn.\n",
      "Things in report: {('996M', 'WLF')}\n",
      "Places in report: {'Peregrines'} \n",
      "\n",
      "paragraph #9\n",
      "Prospect Blacktail ~ Ilona did see a black in the Prospect rendezvous on the Blacktail this morning so not all the pack members were in Little America. Our count was 8 and I think that they are at 14 if they are all together.\n",
      "Things in report: {('Prospect', 'WLFP')}\n",
      "Places in report: {'Blacktail'} \n",
      "\n",
      "paragraph #10\n",
      "Other critters ~ Yesterday, Amy and Brad reported a beautiful bull moose in Round Prairie and another nice bull at Petrified Tree. Today, there was a black bear at Elk Creek. Lots of coyotes between the two carcasses in Lamar.\n",
      "Things in report: {('bear', 'BEAR'), ('moose', 'MOOSE'), ('bull', 'MOOSE')}\n",
      "Places in report: {'Lamar'} \n",
      "\n",
      "paragraph #11\n",
      "paragraph #12\n",
      "paragraph #13\n",
      "paragraph #14\n",
      "paragraph #15\n",
      "paragraph #16\n",
      "paragraph #17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for entry in result['report']:\n",
    "    \n",
    "    paragraph = entry.split(\"\\n\")\n",
    "    paragraph = [x for x in paragraph if x != '']\n",
    "    #print(paragraph[:10])\n",
    "    \n",
    "    for index, i in enumerate(paragraph):\n",
    "        print('paragraph #' + str(index + 1))\n",
    "        #print(i)\n",
    "        #print('testing')\n",
    "        entity_names = []\n",
    "\n",
    "        #print(entry,'...\\n\\n---------------------\\n')\n",
    "        tagged_sentences = []\n",
    "        _POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sent_list = sent_detector.tokenize(i.strip())   \n",
    "\n",
    "        tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sent_list]\n",
    "\n",
    "        model = mydict\n",
    "\n",
    "        tagger = nltk.tag.UnigramTagger(model=model, backoff=default_tagger)\n",
    "        tagged_sentences = [tagger.tag(sentence) for sentence in tokenized_sentences]\n",
    "        chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "        things = []\n",
    "        places = []\n",
    "\n",
    "        for sentence in tagged_sentences:\n",
    "\n",
    "            for item in sentence:\n",
    "\n",
    "                if item[1] == 'WLFP' or item[1] == 'WLF'or item[1] == 'MOOSE'or item[1] == 'BEAR':\n",
    "                    things.append(item) \n",
    "\n",
    "                if item[1] == 'PLC':\n",
    "                    places.append(item[0]) \n",
    "\n",
    "        if len(things) > 0 or len(places) > 0:\n",
    "            print(i)\n",
    "            print(\"Things in report:\", set(things))\n",
    "            print(\"Places in report:\", set(places),\"\\n\")\n",
    "\n",
    "\n",
    "        #for word in chunked_sentences:\n",
    "        #    entity_names.extend(extract_entity_names(word)) \n",
    "        #print(\"potential other objects and places\",entity_names)\n",
    "    \n",
    "    break\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import nltk\n",
    "\n",
    "sentence = \"At eight o'clock on Thursday film morning word line test best beautiful Ram Aaron design\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tagged)\n",
    "length = len(tagged) - 1\n",
    "\n",
    "a = list()\n",
    "\n",
    "for item in tagged:\n",
    "    if item[1][0] == 'N':\n",
    "      a.append(item[0])\n",
    "    \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
    "default_tagger = nltk.data.load(_POS_TAGGER)\n",
    "model = {'Hi': 'VB'}\n",
    "tagger = nltk.tag.UnigramTagger(model=model, backoff=default_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The one that works\n",
    "\n",
    "import nltk.data, nltk.tag\n",
    "\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "class BackoffTagger:\n",
    "    def __init__(self):\n",
    "        self._taggers = [PerceptronTagger()]\n",
    "\n",
    "entity_names = []\n",
    "\n",
    "for entry in result['report']:\n",
    "    tagged_sentences = []\n",
    "    \n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sent_list = sent_detector.tokenize(text.strip())   \n",
    "    \n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sent_list]\n",
    "    \n",
    "\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "    \n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    \n",
    "    for word in chunked_sentences:\n",
    "        print(word)\n",
    "        entity_names.extend(extract_entity_names(word)) \n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
